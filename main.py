from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import google.generativeai as genai
from sentence_transformers import SentenceTransformer, util
import torch
import numpy as np
import pickle
import re

app = FastAPI()

# CORS í—ˆìš©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ë°°í¬ì‹œ ì œí•œí•´ë„ ë¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í™˜ê²½ ë³€ìˆ˜
API_KEY = "YOUR_GOOGLE_API_KEY"  # ğŸ” ë°˜ë“œì‹œ ë³¸ì¸ KEYë¡œ êµì²´
genai.configure(api_key=API_KEY)
gemini_model = genai.GenerativeModel(model_name="models/gemini-2.0-flash")

# ëª¨ë¸ ë° ë°ì´í„° ë¡œë”©
with open("rag_chunks.pkl", "rb") as f:
    chunks = pickle.load(f)

embeddings = np.load("rag_embeddings.npy")
embeddings_tensor = torch.tensor(embeddings)
embedder = SentenceTransformer("jhgan/ko-sroberta-multitask")

class SentenceInput(BaseModel):
    sentence: str

@app.post("/api/correct")
def correct_text(item: SentenceInput):
    try:
        input_sentence = item.sentence
        sentences = re.split(r'[!.?]', input_sentence)
        prompt_sentences = []

        for i, sentence in enumerate(sentences):
            if not sentence.strip():
                continue
            question_embedding = embedder.encode(sentence, convert_to_tensor=True).cpu()
            cos_scores = util.cos_sim(question_embedding, embeddings_tensor)[0]
            top_results = torch.topk(cos_scores, k=3)
            retrieved_chunks = [chunks[idx] for idx in top_results.indices]

            ref_text = (
                f"{i+1}. ë¬¸ì¥: {sentence.strip()}\n"
                f"ì°¸ê³  ë¬¸ë‹¨:\n" + "\n".join(retrieved_chunks)
            )
            prompt_sentences.append(ref_text)

        final_prompt = (
            "ë‹¤ìŒ ë¬¸ì¥ê³¼ ë¬¸ë‹¨ì„ ì°¸ê³ í•´ì„œ ë§ì¶¤ë²•ì„ êµì •í•´ ì£¼ì„¸ìš”. temperatureë¥¼ ê³ ë ¤í•´ì„œ ë¬¸ë§¥ê³¼ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ê³ , ì‹ ì¡°ì–´ë‚˜ í‹€ë¦¬ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”.\n\n"
            + "\n\n".join(prompt_sentences)
            + "\n\nê° ë¬¸ì¥ì— ëŒ€í•´ 'ë¬¸ì¥:'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ë§ˆë‹¤ í•˜ë‚˜ì”© êµì •í•´ ì£¼ì„¸ìš”. ë¬¸ì¥ì˜ ìˆœì„œëŠ” ë²ˆí˜¸ ìˆœì„œì™€ ê°™ê²Œ í•´ ì£¼ì„¸ìš”. ë‹µë³€ì€ ë²ˆí˜¸ëŠ” ë§¤ê¸°ì§€ ë§ê³  'ë¬¸ì¥ :'ì„ ì œê±°í•˜ê³  ì­‰ ì´ì–´ì„œ ë‹µí•´ ì£¼ì„¸ìš”."
        )

        response = gemini_model.generate_content(
            final_prompt,
            generation_config=genai.types.GenerationConfig(temperature=0.0)
        )

        if response and response.text:
            return {"corrected": response.text}
        else:
            return {"corrected": "âš ï¸ êµì • ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    except Exception as e:
        return {"corrected": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
